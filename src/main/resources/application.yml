server:
  port: 8092

spring:
  application:
    name: rag-system
  servlet:
    multipart:
      enabled: true
      max-file-size: 100MB
      max-request-size: 100MB
      file-size-threshold: 2MB

# Configurazione Qdrant
qdrant:
  host: localhost
  port: 6334
  collection-name: documenti
  use-tls: false

# Configurazione RAG
rag:
  # Numero di chunks da recuperare per ogni query
  # Valori consigliati:
  #  5-10:  Veloce, buono per documenti semplici
  #  10-20: Più contesto, meglio per domande complesse
  #  20+:   Massimo contesto, ma più lento e più token usati
  top-k: 15
  
  # Dimensione dei chunks per la suddivisione del testo
  # Valori più bassi = match più precisi ma più chunks totali
  # Valori più alti = meno chunks ma context più grande per chunk
  chunk-size: 300
  chunk-overlap: 50
  
  # Dimensione batch per processamento file grandi
  # Chunks processati contemporaneamente (embedding + salvataggio Qdrant)
  # Valori più bassi = meno memoria usata, più lento
  # Valori più alti = più veloce ma usa più memoria
  # Consigliato: 50-100 per file grandi, 200+ per file piccoli
  batch-size: 50

# Configurazione File Polling con Apache Camel
file-polling:
  # Abilita/disabilita il polling automatico
  enabled: true
  
  # Directory da monitorare per nuovi file
  # Usa path assoluto o relativo alla working directory
  input-directory: rag-input
  
  # Directory dove spostare i file processati (opzionale)
  # Se non specificato, i file rimangono nella stessa directory
  processed-directory: rag-processed
  
  # Directory per i file che hanno dato errore
  error-directory: rag-errors
  
  # Pattern dei file da processare (regex Java)
  # Default: tutti i file PDF, DOC, DOCX, XLS, XLSX, PPT, PPTX, TXT
  file-pattern: .*\\.(pdf|doc|docx|xls|xlsx|ppt|pptx|txt|html|xml)$
  
  # Frequenza di polling in millisecondi (default: 5000ms = 5 secondi)
  delay: 5000
  
  # Processa anche file già esistenti all'avvio? (true/false)
  initial-delay: 1000
  
  # Numero di file da processare in parallelo
  max-concurrent: 3

# Configurazione Google Gemini
gemini:
  # API KEY di Google AI Studio (https://aistudio.google.com/app/apikey)
  api-key: ${GEMINI_API_KEY:your_api_key_here}
  
  # Se usi Vertex AI (GCP), configura questi:
  project: ${GOOGLE_CLOUD_PROJECT:}
  location: us-central1
  
  # Modello da usare
  # Opzioni: gemini-2.0-flash-exp, gemini-1.5-flash, gemini-1.5-pro
  model: gemini-2.5-flash
  
  # Parametri di generazione
  # Temperature: controlla la creatività delle risposte
  #   0.0-0.3: Deterministico, preciso, ideale per FAQ/documentazione
  #   0.3-0.7: Bilanciato, buono per RAG generale
  #   0.7-1.0: Creativo, può aggiungere dettagli non nel contesto
  temperature: 0.3
  
  # Max tokens: lunghezza massima della risposta
  #   512-1024:  Risposte brevi e concise
  #   1024-2048: Risposte medie (consigliato per RAG)
  #   2048-4096: Risposte lunghe e dettagliate
  max-tokens: 1024

# Logging
logging:
  level:
    root: INFO
    com.example.rag: DEBUG
    dev.langchain4j: DEBUG
  pattern:
    console: "%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n"
