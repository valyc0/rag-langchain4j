server:
  port: 8092

spring:
  application:
    name: rag-system
  profiles:
    active: gemini
  servlet:
    multipart:
      enabled: true
      max-file-size: 100MB
      max-request-size: 100MB
      file-size-threshold: 2MB

# Configurazione Qdrant
qdrant:
  host: localhost
  port: 6334
  collection-name: documenti
  use-tls: false

# Configurazione RAG
rag:
  # Numero di chunks da recuperare per ogni query
  # Valori consigliati:
  #  5-10:  Veloce, buono per documenti semplici
  #  10-20: Più contesto, meglio per domande complesse
  #  20+:   Massimo contesto, ma più lento e più token usati
  top-k: 15
  
  # Dimensione dei chunks per la suddivisione del testo
  # Valori più bassi = match più precisi ma più chunks totali
  # Valori più alti = meno chunks ma context più grande per chunk
  chunk-size: 300
  chunk-overlap: 50

# Configurazione File Polling con Apache Camel
file-polling:
  # Abilita/disabilita il polling automatico
  enabled: true
  
  # Directory da monitorare per nuovi file
  # Usa path assoluto o relativo alla working directory
  input-directory: rag-input
  
  # Directory dove spostare i file processati (opzionale)
  # Se non specificato, i file rimangono nella stessa directory
  processed-directory: rag-processed
  
  # Directory per i file che hanno dato errore
  error-directory: rag-errors
  
  # Pattern dei file da processare (regex Java)
  # Default: tutti i file PDF, DOC, DOCX, XLS, XLSX, PPT, PPTX, TXT
  file-pattern: .*\\.(pdf|doc|docx|xls|xlsx|ppt|pptx|txt|html|xml)$
  
  # Frequenza di polling in millisecondi (default: 5000ms = 5 secondi)
  delay: 5000
  
  # Processa anche file già esistenti all'avvio? (true/false)
  initial-delay: 1000
  
  # Numero di file da processare in parallelo
  max-concurrent: 3

# ============================================
# CONFIGURAZIONE LLM (Language Model)
# ============================================
# Provider supportati: gemini, ollama, openrouter
llm:
  # Provider da utilizzare: gemini | ollama | openrouter
  provider: ${LLM_PROVIDER:gemini}
  
  # Parametri comuni a tutti i provider
  temperature: 0.3
  max-tokens: 1024

# Configurazione Google Gemini
gemini:
  # API KEY di Google AI Studio (https://aistudio.google.com/app/apikey)
  api-key: ${GEMINI_API_KEY:your_api_key_here}
  
  # Se usi Vertex AI (GCP), configura questi:
  project: ${GOOGLE_CLOUD_PROJECT:}
  location: us-central1
  
  # Modello da usare
  # Opzioni: gemini-2.0-flash-exp, gemini-1.5-flash, gemini-1.5-pro, gemini-2.5-flash
  model: gemini-2.5-flash

# Configurazione Ollama (modelli locali)
ollama:
  # URL del server Ollama (default: localhost:11434)
  base-url: ${OLLAMA_BASE_URL:http://localhost:11434}
  
  # Modello da usare (deve essere già scaricato con: ollama pull <modello>)
  # Opzioni popolari: llama3.2:1b, llama3.2, llama3.1, mistral, mixtral, codellama, phi3, qwen2
  model: ${OLLAMA_MODEL:llama3.2:1b}
  
  # Timeout in secondi (i modelli locali possono essere lenti)
  timeout: 300

# Configurazione OpenRouter (gateway multi-LLM)
openrouter:
  # API KEY di OpenRouter (https://openrouter.ai/keys)
  api-key: ${OPENROUTER_API_KEY:your_openrouter_key_here}
  
  # Modello da usare (vedi https://openrouter.ai/models per lista completa)
  # Esempi:
  #   - openai/gpt-4-turbo
  #   - openai/gpt-3.5-turbo
  #   - anthropic/claude-3-opus
  #   - anthropic/claude-3-sonnet
  #   - anthropic/claude-3-haiku
  #   - google/gemini-pro
  #   - meta-llama/llama-3-70b-instruct
  #   - mistralai/mixtral-8x7b-instruct
  #   - qwen/qwen-72b-chat
  model: ${OPENROUTER_MODEL:x-ai/grok-4.1-fast:free}
  
  # Nome della tua applicazione (opzionale, per tracking)
  app-name: ${OPENROUTER_APP_NAME:RAG-System}
  
  # Sito/URL della tua applicazione (opzionale)
  app-url: ${OPENROUTER_APP_URL:}

# Logging
logging:
  level:
    root: INFO
    com.example.rag: DEBUG
    dev.langchain4j: DEBUG
  pattern:
    console: "%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n"
